{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_t_1 = f\"https://www.decitre.fr/livres/litterature/\"\n",
    "s_t_2 = \"/meilleures-ventes.html\"\n",
    "genrelist = ['fantasy-sf', 'romans', 'livres-audio', 'pleiades', 'polars']\n",
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "broth = []\n",
    "for genre in genrelist:\n",
    "    url = s_t_1 + genre +s_t_2\n",
    "    ua = {'User-agent': 'Mozilla/5.0'}\n",
    "    page = requests.get(url, headers=ua, timeout=30)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    broth.append(soup)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ for each genre :\n",
    " + extract the name, price, and review score of n books (n being an input parameter of your\n",
    " program ; note that you may have to go through pagination) ;\n",
    " + for each book:\n",
    " + search for a Wikipedia article matching the authors name\n",
    " + extract the full text from this article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(broth[0].find_all('div', class_=\"content-50 catalog-product-list-details fiche_page_recherche\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class=\"content-50 catalog-product-list-details fiche_page_recherche\"\n",
    "# a = []\n",
    "# t = []\n",
    "# r = []\n",
    "l = []\n",
    "for para in broth[0].find_all('div', class_=\"content-50 catalog-product-list-details fiche_page_recherche\"):\n",
    "            title = para.find('a', class_=\"product-title\").text.strip()\n",
    "            author = para.find('div', class_='authors').text.strip()\n",
    "            susurl = para.find('a', class_=\"product-title\")[\"href\"]\n",
    "            susu = requests.get(susurl, headers=ua, timeout=30)\n",
    "            susup = BeautifulSoup(susu.content, 'html.parser')\n",
    "            resume = susup.find('div', class_=\"content\").text.strip()\n",
    "            d = {\"title\": title, \"author\": author, \"resume\": resume}\n",
    "            l.append(d)\n",
    "            # a.append(author)\n",
    "            # t.append(title)\n",
    "            # r.append(resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "susurl = para.find('a', class_=\"product-title\")[\"href\"]\n",
    "susu = requests.get(susurl, headers=ua, timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "susup = BeautifulSoup(susu.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"content\">\n",
       "            L'humanité sait qu'il lui reste quatre siècles avant que la flotte trisolarienne n'envahisse le système solaire. Les sciences fondamentales se retrouvant verrouillées par les intellectrons, la Terre doit se préparer du mieux qu'elle peut. Le Conseil de Défense Planétaire lance un nouveau projet : le programme Colmateur, qui consiste à faire appel à quatre individus chargés d'envisager des stratégies secrètes pour contrer l'invasion ennemie. Car s'ils peuvent espionner toutes les conversations et tous les ordinateurs humains grâce aux intellectrons, les Trisolariens sont en revanche incapables de lire dans les pensées. \"La Forêt sombre\" est le deuxième volume de la trilogie du \"Problème à trois corps\" devenue culte, adaptée pour Netflix en 2024 par les créateurs de \"Game of Thrones\".        </div>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sssss = susup.find('div', class_=\"content\")\n",
    "sssss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HTML file\n",
    "with open(\"Moderne Contemporain _ Les Arts décoratifs.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "# Find all <a> tags containing both href and title attributes, along with an <img> tag\n",
    "links = soup.find_all(\"a\", href=True, title=True)\n",
    "\n",
    "# Extract href, title, and src from the tags\n",
    "results = []\n",
    "for link in links:\n",
    "    img_tag = link.find(\"img\", src=True)  # Look for an <img> tag inside the <a> tag\n",
    "    if img_tag:\n",
    "        href = link[\"href\"]\n",
    "        title = link[\"title\"]\n",
    "        src = img_tag[\"src\"]\n",
    "        results.append({\"href\": href, \"title\": title, \"src\": src})\n",
    "\n",
    "# Display the results\n",
    "for result in results:\n",
    "    print(f\"Href: {result['href']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Src: {result['src']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(url):\n",
    "    # authors = []\n",
    "    authors = {}\n",
    "    for i in range(1, 11):\n",
    "        url = f'http://quotes.toscrape.com/page/{i}/'\n",
    "        # Get page located at url:\n",
    "        ua = {'User-agent': 'Mozilla/5.0'}\n",
    "        page = requests.get(url, headers=ua)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        #Get all links corresponding to authors:\n",
    "        for para in soup.find_all('div', class_='quote'):\n",
    "            author = para.find('small', class_='author').text.strip()\n",
    "            if authors.get(author) == None:\n",
    "                link_href = para.find('a')['href']\n",
    "                authors[author] = BASE_URL+link_href\n",
    "    # title_tag = para.find(\"a\", title=True)\n",
    "    # link = title_tag.get(\"href\")\n",
    "    # print(title_tag, link)\n",
    "    # for para in soup.find_all('quote'):\n",
    "    #     title_tag = para.find(\"a\", title=True)\n",
    "    #     link = title_tag.get(\"href\")\n",
    "    \n",
    "    #Loop over these:\n",
    "    \n",
    "        #if a link is not in authors, add it:\n",
    "        \n",
    "    #Return results\n",
    "    return authors\n",
    "\n",
    "#Test:\n",
    "authors = get_links(BASE_URL)\n",
    "print(authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "\n",
    " Extend your python program to turn the dataset compiled in exercise 1 into a JSON le structured\n",
    " as follows:\n",
    " {\n",
    " }\n",
    " books: [\n",
    " { name: <string>,\n",
    " description: <string>,\n",
    " price: <number>,\n",
    " score: <number>,\n",
    " genre: <string>,\n",
    " author: <string>,\n",
    " author_bio: <string>\n",
    " }\n",
    " , ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    " Set up a mongoDB noSQL database containing a collection named books which will be lled with\n",
    " information coming from the JSON le compiled in exercise 2. Please note that you may have to\n",
    " modify the JSON structure to have separate entries for each product in the collection.\n",
    " Please write down the commands used to set up the database in a markdown cell in your jupyter\n",
    " notebook, and extend your Python notebook to feed the mongoDB database with the pieces of\n",
    " information extracted in exercise 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "Extend your Python program to extract the following pieces of information from your database :\n",
    " • how many books are there?\n",
    " • how many books with a score greater than 3?\n",
    " • how many books with a description of more than 50 words long?\n",
    " • how many books with a price less than 10 EUR?\n",
    " • which book is the most expensive?\n",
    " • which books have a score bigger than 3 and a description longer than 50 words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
